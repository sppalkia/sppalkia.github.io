<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <title>
    
      Snorkel and The Dawn of Weakly Supervised Machine Learning &middot; Stanford DAWN
    
  </title>
  <link rel="stylesheet" href="/dawn/styles.css">
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/dawn/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/dawn/public/favicon.ico">
  <link rel="alternate" type="application/atom+xml" title="Stanford DAWN" href="/dawn/atom.xml">
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
</head>


  <body>

    <div class="container content">
      <header class="masthead">
        <h3 class="masthead-title" style="text-align:center">
           <a href="/dawn/" title="Home"><span id="header-stanford">Stanford </span>DAWN</a>
        </h3>
      </header>

      <img src="/dawn/assets/quad_center.jpg">

    <nav id="menu">
        <a  href="/dawn/people/">People</a>
            <a  href="/dawn/members/">Members</a>
            <a  href="/dawn/projects/">Projects</a>
            <a  href="/dawn/blog/">Blog</a>
            <a  href="/dawn/whitepaper/">Whitepaper</a>
            <a  href="/dawn/seminar/">Seminar</a>
    </nav>

      <main>
        <article class="post">
  <h1 class="post-title">Snorkel and The Dawn of Weakly Supervised Machine Learning</h1>
  <i><h7 class="post-author">by Alex Ratner, Stephen Bach, Henry Ehrenberg, and Chris Ré</h7></i>
  <time datetime="2017-05-08T00:00:00-07:00" class="post-date">08 May 2017</time>
  <p>In this post, we’ll discuss our approaches to weakly supervising complex machine learning models in the age of big data. Learn more about Snorkel, our system for rapidly creating training sets with weak supervision, at <a href="http://snorkel.stanford.edu">snorkel.stanford.edu</a>.</p>

<h2 id="labeled-training-data-the-new-new-oil">Labeled Training Data: The <em>New</em> New Oil</h2>

<p><img src="/dawn/assets/img/2017-05-08-snorkel/new-new-banner.png" align="middle" /></p>

<p>Today’s state-of-the-art machine learning models are both more powerful and easier to spin up than ever before.
Whereas practitioners used to spend the bulk of their time carefully engineering <em>features</em> for their models, we can now feed in raw data - images, text, genomic sequences, etc. - to systems that <em>learn</em> their own features.
These powerful models, like deep neural networks, produce state-of-the-art results on many tasks.
This new power and flexibility has sparked excitement about machine learning in fields ranging from medicine to business to law.</p>

<p><strong>There is a hidden cost to this success, however: these models require <em>massive</em> labeled training sets.</strong>
And while machine learning researchers can use carefully manicured training sets to benchmark their new models, these labeled training sets do not exist for most real world tasks.
Creating sufficiently large labeled training datasets is extremely expensive and slow in practice.
This is exacerbated when <strong>domain expertise is required</strong> to label data, such as a radiologist labeling MRI images as containing malignant tumors or not.
In addition, <strong>hand-labeled training data is not adaptable or flexible</strong>, and thus entirely unsuitable for learning tasks which change over time.</p>

<h2 id="weak-supervision">Weak Supervision</h2>
<p>We’re quite excited about a set of approaches broadly termed <em>weak supervision</em> to address the bottleneck described above.
The basic idea is to <strong>allow users to provide supervision at a higher level</strong> than case-by-case labeling, and then to <strong>use various statistical techniques to deal with the noisier labels</strong> that we get.
Working with domain experts, we found that this type of supervision is easier and faster to provide.
And surprisingly, <strong>by getting large volumes of lower-quality supervision in this way, we can train higher-quality models at a fraction of the time and cost.</strong>
We see weak supervision-based systems as one of the most exciting directions in terms of how users will train, deploy, and interact with machine learning systems.</p>

<p><img src="/dawn/assets/img/2017-05-08-snorkel/dp_workflow.png" align="middle" /></p>

<p>One of the main techniques that we are currently developing in this direction is called <strong><em>data programming</em></strong> (see our blog post about it <a href="http://hazyresearch.github.io/snorkel/blog/weak_supervision.html">here</a>, or the NIPS 2016 paper <a href="https://papers.nips.cc/paper/6523-data-programming-creating-large-training-sets-quickly">here</a>).
In the data programming paradigm, users focus on <strong>writing a set of <em>labeling functions</em></strong>, which are just small functions that programmatically label data.
The labels that they produce are noisy and could conflict with each other.
However, <strong>we can model this noise</strong> by learning a generative model of the labeling process, effectively synthesizing the labels created by the labeling functions.
We can then use this new label set to train a <em>noise-aware</em> end discriminative model (such as a neural network in TensorFlow) with higher accuracy.
<strong>This framework allow users to easily “program” machine learning models with high-level functions</strong>, and leverage whatever code, domain heuristics, or data resources they have at hand.
And, since the supervision is provided as functions, it allows us to <em>scale with the amount of unlabeled data</em>!</p>

<h2 id="snorkel">Snorkel</h2>
<p><img src="/dawn/assets/img/2017-05-08-snorkel/snorkel-1.png" align="middle" /></p>

<p><strong><a href="http://snorkel.stanford.edu">Snorkel</a> is a system built around the data programming paradigm for rapidly creating, modeling, and managing training data.</strong>
Snorkel is currently focused on accelerating the development of structured or “dark” data extraction applications for domains in which large labeled training sets are not available or easy to obtain.
For example, Snorkel is being currently used on text extraction applications <strong>on medical records at the Deparment of Veterans Affairs</strong>, to mine scientific literature for <strong>adverse drug reactions in collaboration with the Federal Drug Administration</strong>, and to comb through everything from <strong>surgical reports to after-action combat reports</strong> for valuable structured data.</p>

<h2 id="and-beyond">…And Beyond</h2>
<p>We’ve been working hard on next steps for data programming, Snorkel, and other weak supervision techniques, some of which we’ve already posted about:</p>

<ul>
  <li>
    <p><a href="http://hazyresearch.github.io/snorkel/blog/structure_learning.html">Structure learning</a>: How can we detect correlations and other statistical dependencies among labeling functions? Modeling these dependencies are important because a misspecified generative model can lead to misestimating the labeling functions’ accuracies. We’ve <a href="https://arxiv.org/abs/1703.00854">proposed a method</a> that can quickly identify dependencies without any ground truth data.</p>
  </li>
  <li>
    <p><a href="http://hazyresearch.github.io/snorkel/blog/socratic_learning.html">Socratic learning</a>: How can we more effectively model and debug the user-written labeling functions in data programming?  We’re working on <a href="https://arxiv.org/abs/1610.08123">a method</a> to use differences between the generative and discriminative models to help do this.</p>
  </li>
  <li>
    <p><a href="https://hazyresearch.github.io/snorkel/blog/fonduer.html">Semi-structured data extraction</a>: How can we handle extracting structured data from data that has <em>some</em> structure such as tables embedded in PDFs and webpages?  We’ve been working on a system called <a href="https://arxiv.org/abs/1703.05028">Fonduer</a> to make this fast and easy in Snorkel!</p>
  </li>
  <li>
    <p><a href="https://hazyresearch.github.io/snorkel/blog/babble_labble.html">Learning from natural language supervision</a>: Can we use natural language as a form of weak supervision, parsing the semantics of natural language statements and then using these as labeling functions?  We’ve done some exciting preliminary work here!</p>
  </li>
</ul>

</article>


<aside class="related">
  <h3>Related posts</h3>
  <ul class="related-posts">
    
      <li>
        <a href="/dawn/2017/05/12/holoclean/">
          HoloClean - Weakly Supervised Data Repairing
          <small><time datetime="2017-05-12T00:00:00-07:00">12 May 2017</time></small>
        </a>
      </li>
    
      <li>
        <a href="/dawn/2017/04/28/nsdi/">
          A retrospective on NSDI 2017
          <small><time datetime="2017-04-28T00:00:00-07:00">28 Apr 2017</time></small>
        </a>
      </li>
    
      <li>
        <a href="/dawn/2017/04/25/weld/">
          Implementing Weld in Rust
          <small><time datetime="2017-04-25T00:00:00-07:00">25 Apr 2017</time></small>
        </a>
      </li>
    
  </ul>
</aside>


      </main>


          <footer class="footer">
        <small>
          &copy; <time datetime="2017-05-17T15:03:45-07:00">2017</time>. All rights reserved.
        </small>
      </footer>
    </div>


    
  </body>
</html>
